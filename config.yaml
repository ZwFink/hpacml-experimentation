---
particlefilter:
  train_script: './train.py'
  train_args:
    data_args:
      region_name: 'particlefilter'
      load_entire: false
      train_test_dataset: '/scratch/mzu/zanef2/surrogates/training_data/particlefilter_128x128_13000.h5'
    early_stop_args:
      patience: 10
      min_delta_percent: 3
  bayesian_opt_driver_args:
    architecture_config:
      trials: 100
      parameter_constraints:
      - "maxpool_kernel_size <= conv_kernel_size"
      - "conv_stride <= conv_kernel_size"
      objectives:
      - name: average_mse
        type: minimize
      - name: inference_time
        type: minimize
        threshold: 3
      parameter_space:
      - name: conv_kernel_size
        type: range
        bounds:
        - 2
        - 14
      - name: conv_stride
        type: range
        bounds:
        - 2
        - 14
      - name: maxpool_kernel_size
        type: range
        bounds:
        - 1
        - 10
      - name: fc2_size
        type: range
        bounds:
        - 0 
        - 128
      tracking_metrics:
      - average_mse
      - inference_time
  
    hyperparameter_config:
      trials: 30
      parameter_space:
      - name: learning_rate
        type: range
        bounds:
        - 0.0001
        -  0.01
        log_scale: true
      - name: epochs
        type: fixed
        value: 500
      - name: batch_size
        type: fixed
        value: 32
      - name: dropout
        type: fixed
        value: 0
      - name: weight_decay
        type: range
        bounds:
        - 0.0001
        - 0.1
        log_scale: true
      objectives:
      - name: average_mse
        type: minimize
      tracking_metrics:
      - average_mse
miniweather:
  train_script: './train.py'
  train_args:
    data_args:
      region_name: 'MiniWeatherTimestep'
      train_test_dataset: '/scratch/mzu/zanef2/surrogates/training_data/miniweather_768_384.h5'
      load_entire: false
      train_end_index: 1000
    early_stop_args:
      patience: 5
      min_delta_percent: 2
  bayesian_opt_driver_args:
    architecture_config:
      trials: 100
      objectives:
      - name: average_mse
        type: minimize
      - name: inference_time
        type: minimize
        threshold: 10
      parameter_space:
      - name: conv1_stride
        type: fixed
        value: 1
      - name: conv1_kernel_size
        type: range
        bounds:
        - 2
        - 8 
      - name: conv1_out_channels
        type: range
        bounds:
        - 4
        - 8
      - name: conv2_kernel_size
        type: range
        bounds:
        - 0
        - 6
      - name: activation_function
        type: fixed
        value: leaky_relu
      tracking_metrics:
      - average_mse
      - inference_time
    hyperparameter_config:
      trials: 30
      parameter_constraints:
        - "inference_time <= 10"
      parameter_space:
      - name: learning_rate
        type: range
        bounds:
        - 0.0001
        -  0.01
        log_scale: true
      - name: dropout
        type: range
        bounds:
        - 0.0
        - 0.5
      - name: epochs
        type: fixed
        value: 100
      - name: batch_size
        type: fixed
        value: 32
      - name: weight_decay
        type: range
        bounds:
        - 0.0001
        - 0.1
        log_scale: true
      objectives:
      - name: average_mse
        type: minimize
      tracking_metrics:
      - average_mse
binomial_options:
  train_script: './train.py'
  train_args:
    data_args:
      region_name: 'BinomialOptionsKernel'
      train_test_dataset: '/scratch/mzu/zanef2/surrogates/training_data/binomial_options_16777216.h5'
      load_entire: true
      train_end_index: 6194304 
      # We have so much data for this one,
      # performance is affected a lot by 
      # the amount of test data
      test_split: 0.97
      max_test_batches: 8000
    early_stop_args:
      patience: 5
      min_delta_percent: 1
  bayesian_opt_driver_args:
    architecture_config:
      trials: 100
      parameter_constraints:
      - "hidden2_features <= hidden1_features"
      objectives:
      - name: average_mse
        type: minimize
      - name: inference_time
        type: minimize
      parameter_space:
      - name: multiplier
        type: range
        bounds: 
        - 1
        - 32
      - name: hidden1_features
        type: range
        bounds:
        - 5
        - 32
      - name: hidden2_features
        type: range
        bounds:
        - 0
        - 32
      tracking_metrics:
      - average_mse
      - inference_time
    hyperparameter_config:
      trials: 40
      parameter_constraints:
        - "inference_time <= 4"
      parameter_space:
      - name: learning_rate
        type: range
        bounds:
        - 0.0001
        -  0.01
        log_scale: true
      - name: epochs
        type: fixed
        value: 100
      - name: batch_size
        type: fixed
        value: 32
      - name: dropout
        type: range
        bounds:
        - 0.0
        - 0.1
      - name: weight_decay
        type: range
        bounds:
        - 0.0001
        - 0.1
        log_scale: true
      objectives:
      - name: average_mse
        type: minimize
      tracking_metrics:
      - average_mse
bonds:
  train_script: './train.py'
  train_args:
    data_args:
      region_name: 'BondsKernel'
      train_test_dataset: '/scratch/mzu/zanef2/surrogates/training_data/bonds_16777216.h5'
      load_entire: true
      train_end_index: 6194304 
      # We have so much data for this one,
      # performance is affected a lot by 
      # the amount of test data
      test_split: 0.95
      # Admittedly, this is awkward.
      # We don't want to use our entire 'test set'
      # (non training set) for validation, but we
      # do want to use the entire thing for measuring
      # inference tim
      max_test_batches: 8000
    early_stop_args:
      patience: 10
      min_delta_percent: 1
  bayesian_opt_driver_args:
    architecture_config:
      trials: 100
      parameter_constraints:
      - "hidden2_features <= hidden1_features"
      objectives:
      - name: average_mse
        type: minimize
      - name: inference_time
        type: minimize
      parameter_space:
      - name: multiplier
        type: range
        bounds: 
        - 1
        - 32
      - name: hidden1_features
        type: range
        bounds:
        - 5
        - 32
      - name: hidden2_features
        type: range
        bounds:
        - 0
        - 32
      tracking_metrics:
      - average_mse
      - inference_time
    hyperparameter_config:
      trials: 50
      parameter_constraints:
        - "inference_time <= 8"
      parameter_space:
      - name: learning_rate
        type: range
        bounds:
        - 0.0001
        -  0.01
        log_scale: true
      - name: epochs
        type: fixed
        value: 30
      - name: batch_size
        type: fixed
        value: 32
      - name: dropout
        type: range
        bounds:
        - 0.0
        - 0.5
      - name: weight_decay
        type: range
        bounds:
        - 0.0001
        - 0.1
        log_scale: true
      objectives:
      - name: average_mse
        type: minimize
      tracking_metrics:
      - average_mse
minibude:
  train_script: './train.py'
  train_args:
    data_args:
      region_name: 'BUDEKernel'
      # Allow us to specialize the data location
      # in case jobs share the same node
      # (though we don't run them in parallel)
      train_test_dataset: '/tmp/bude_data_%s.h5'
      datagen_args:
        benchmark_location: '/scratch/mzu/zanef2/surrogates/SurrogateBenchmarks/benchmarks/miniBUDE/'
        dataset_gen_command:
          - 'bude'
          - '--deck'
          - '/scratch/mzu/zanef2/miniBUDE_data/bm2'
          - '-i'
          - '1'
          - '-n'
          - '2097152'
          - '--ni'
          - '4'
      load_entire: false
      test_split: 0.2
    early_stop_args:
      patience: 10
      min_delta_percent: 1
  bayesian_opt_driver_args:
    architecture_config:
      trials: 100
      # Here we use a multiplier instead of number of features
      # because the number of input features id a dependent
      # variable that's challenging to represent here
      objectives:
      - name: average_mse
        type: minimize
        threshold: 10
      - name: inference_time
        type: minimize
        threshold: 5000
      parameter_space:
      - name: feature_multiplier
        type: range
        bounds: 
        - 0.1
        - 1.0
      - name: multiplier
        type: range
        bounds:
        - 32
        - 256
      - name: num_hidden_layers
        type: range
        bounds:
        - 2
        - 8 
      - name: hidden_1_features
        type: fixed
        value: 22320
      tracking_metrics:
      - average_mse
      - inference_time
    hyperparameter_config:
      trials: 30
      parameter_space:
      - name: learning_rate
        type: range
        bounds:
        - 0.000001
        -  0.01
        log_scale: true
      - name: epochs
        type: fixed
        value: 70
      - name: batch_size
        type: range
        bounds:
          - 32
          - 512
      - name: dropout
        type: range
        bounds:
        - 0.0
        - 0.8
      - name: weight_decay
        type: range
        bounds:
        - 0.0001
        - 0.1
        log_scale: true
      objectives:
      - name: average_mse
        type: minimize
      tracking_metrics:
      - average_mse
